{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60026fdb",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9f76d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import queue\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "from knn_model_builder import build_knn_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da24d7",
   "metadata": {},
   "source": [
    "# Define the node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17f61439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state,h, parent=None, action=None, g=0, f=0, crop_type=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.g = g  # Cumulative cost from start to this node\n",
    "        self.f = f  # Evaluation cost (g + heuristic if applicable)\n",
    "        self.h = h\n",
    "        self.crop_type = crop_type  # Store the crop type associated with this state\n",
    "        # Calculate the depth of the node\n",
    "        if parent is None:\n",
    "            self.depth = 0\n",
    "        else:\n",
    "            self.depth = parent.depth + 1\n",
    "\n",
    "    def __hash__(self):  # Return hash value for each Node to access it from hash tables (e.g., sets)\n",
    "        if isinstance(self.state, list):\n",
    "            state_tuple = tuple([tuple(row) for row in self.state])\n",
    "            return hash(state_tuple)\n",
    "        return hash(self.state)\n",
    "\n",
    "    def __eq__(self, other):  # Helps to check duplicates in the sets\n",
    "        return isinstance(other, Node) and self.state == other.state\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return isinstance(other, Node) and self.f > other.f\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return isinstance(other, Node) and self.f < other.f\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State: {self.state}, Crop: {self.crop_type}, f-value: {self.f}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba60e9",
   "metadata": {},
   "source": [
    "# A* porblem formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a64d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CropsPredictionProblem:\n",
    "    def __init__(self, state_space, goal_state, crop_labels, actions=\"\", path_cost=0):\n",
    "        self.state_space = np.array(state_space)  # Matrix each row represents an environement for the entire crooptypes\n",
    "        self.goal_state = goal_state\n",
    "        self.actions = actions\n",
    "        self.path_cost = path_cost\n",
    "        # the following are some necessary attritbute that are derived from the previouse one\n",
    "        self.crop_labels = crop_labels  # Crop type corresponding to each row\n",
    "        self.distance_calculations = 0 \n",
    "        self.weights = [\n",
    "            0.100,  # N\n",
    "            0.127,  # P\n",
    "            0.153,  # K\n",
    "            0.080,  # temperature\n",
    "            0.188,  # humidity\n",
    "            0.059,  # ph\n",
    "            0.195,  # rainfall\n",
    "            0.008,  # soil_moisture\n",
    "            0.002,  # soil_type\n",
    "            0.009,  # sunlight_exposure\n",
    "            0.009,  # wind_speed\n",
    "            0.009,  # co2_concentration\n",
    "            0.008,  # organic_matter\n",
    "            0.004,  # irrigation_frequency\n",
    "            0.007,  # crop_density\n",
    "            0.008,  # pest_pressure\n",
    "            0.008,  # fertilizer_usage\n",
    "            0.002,  # growth_stage\n",
    "            0.007,  # urban_area_proximity\n",
    "            0.002,  # water_source_type\n",
    "            0.008,  # frost_risk\n",
    "            0.008   # water_usage_efficiency\n",
    "        ]\n",
    "        \n",
    "        # Group environments by crop type\n",
    "        self.crop_environments = defaultdict(list)\n",
    "        self.crop_indices = defaultdict(list)  # Keep track of original indices\n",
    "        for i, crop_type in enumerate(crop_labels):\n",
    "            self.crop_environments[crop_type].append(tuple(state_space[i]))\n",
    "            self.crop_indices[crop_type].append(i)\n",
    "        self.knn_models, self.crop_initial_envs = build_knn_models(self.crop_environments, self.weights)# the knn model represent the state transition model and inital env represent the initial state \n",
    "        self.crop_labels = list(set(crop_labels))\n",
    "    \n",
    "    def get_hcost(self, state):\n",
    "        \"\"\"Calculate heuristic cost (distance to goal state)\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = tuple(state)\n",
    "            \n",
    "        sum_distance = 0\n",
    "        for i in range(len(state)):\n",
    "            sum_distance += abs(state[i] - self.goal_state[i]) * self.weights[i]\n",
    "        \n",
    "        return sum_distance\n",
    "    \n",
    "    def get_gcost(self, parent_state, state, crop_label=None):\n",
    "        \"\"\"Calculate g cost (distance from initial state)\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = tuple(state)\n",
    "        if isinstance(parent_state, np.ndarray):\n",
    "            parent_state = tuple(parent_state)\n",
    "        sum_distance = 0\n",
    "        return sum(\n",
    "            abs(state[i] - parent_state[i]) * self.weights[i]\n",
    "            for i in range(len(state))\n",
    "        )\n",
    "    def expand_node(self, node):\n",
    "        \"\"\"\n",
    "        Optimized expand node function:\n",
    "        - First level: consider all crop types (using their best environment)\n",
    "        - Subsequent levels: stay within the current crop type\n",
    "        \"\"\"\n",
    "        child_nodes = []\n",
    "        current_state = node.state\n",
    "        current_crop_type = node.crop_type\n",
    "        knn_model = self.knn_models[current_crop_type]\n",
    "        environments = np.array(self.crop_environments[current_crop_type])\n",
    "        distances, indices = knn_model.kneighbors([current_state])\n",
    "        for idx in indices[0]:\n",
    "            neighbor_state = tuple(environments[idx])\n",
    "            if neighbor_state == current_state:\n",
    "                continue\n",
    "            g_cost = self.get_gcost(neighbor_state , node.state , current_crop_type)\n",
    "            h_cost = self.get_hcost(neighbor_state)\n",
    "            f_cost = g_cost + h_cost\n",
    "            child_node = Node(\n",
    "                state=neighbor_state,\n",
    "                parent=node,\n",
    "                g=g_cost,\n",
    "                f=f_cost,\n",
    "                h=h_cost,\n",
    "                crop_type=current_crop_type,\n",
    "               action=f\"move_within_{current_crop_type}\"\n",
    "            )\n",
    "            child_nodes.append(child_node)\n",
    "        return child_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c5198",
   "metadata": {},
   "source": [
    "# The A* algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5a0c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarSearch:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.frontier = queue.PriorityQueue()\n",
    "        self.explored = set()\n",
    "        self.best_nodes_by_crop = dict()  #Track best node found for each crop type\n",
    "\n",
    "        # Add these metrics tracking variables\n",
    "        self.nodes_expanded = 0\n",
    "        self.max_frontier_size = 0\n",
    "        self.execution_time = 0\n",
    "\n",
    "    def search(self):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for croptype in self.problem.crop_labels:# we search for each crop type \n",
    "            self.frontier = queue.PriorityQueue()\n",
    "            self.explored = set()\n",
    "            initial_state = self.problem.crop_initial_envs[croptype]\n",
    "            f_cost = self.problem.get_hcost(initial_state)\n",
    "            initial_node = Node(initial_state , g=0 , f=f_cost , crop_type=croptype , h=f_cost )\n",
    "            self.frontier.put((initial_node.f, initial_node))\n",
    "            while not self.frontier.empty() : \n",
    "                _ , node = self.frontier.get()\n",
    "                self.explored.add(node)\n",
    "                self.nodes_expanded += 1\n",
    "                child_nodes = self.problem.expand_node(node)\n",
    "                for child in child_nodes : \n",
    "                    if child in self.explored : \n",
    "                        continue \n",
    "                    self.frontier.put((child.f , child))\n",
    "                # and then check whether the current state has h less than the top of the frontier and than return it\n",
    "                if not self.frontier.empty():\n",
    "                    top_f, top_node = self.frontier.queue[0]\n",
    "                    if node.h < top_node.h:\n",
    "                        self.best_nodes_by_crop[croptype] = node.state\n",
    "                        break\n",
    "                self.max_frontier_size = max(self.max_frontier_size, self.frontier.qsize())\n",
    "            self.execution_time = time.time() - start_time\n",
    "        return self.best_nodes_by_crop \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130ab4e",
   "metadata": {},
   "source": [
    "# Test the A* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c033035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_best_crop(problem):\n",
    "    \"\"\"\n",
    "    Use A* search to find the best crop type based on environment similarity.\n",
    "    Focuses only on finding the closest environment to the user input.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Initialize and run A* search\n",
    "    a_star = AStarSearch(problem)\n",
    "    best_nodes = a_star.search()  # Maps crop types to best environment (state tuple)\n",
    "\n",
    "    # Sort crop types by their h-cost (distance to user input)\n",
    "    sorted_crops = sorted(\n",
    "        [(crop, {'environment': best_nodes[crop], 'score': problem.get_hcost(best_nodes[crop])}) \n",
    "         for crop in best_nodes],\n",
    "        key=lambda x: x[1]['score']\n",
    "    )\n",
    "\n",
    "    # Recommended crop is the one with lowest h-cost\n",
    "    recommended_crop = sorted_crops[0][0] if sorted_crops else None\n",
    "\n",
    "    # Format crop scores for output\n",
    "    crop_scores = {\n",
    "        crop: {\n",
    "            'environment': best_nodes[crop],\n",
    "            'score': problem.get_hcost(best_nodes[crop])\n",
    "        } for crop in best_nodes\n",
    "    }\n",
    "\n",
    "    # Format ranked crops\n",
    "    ranked_crops = []\n",
    "    for i, (crop, data) in enumerate(sorted_crops):\n",
    "        ranked_crops.append({\n",
    "            'rank': i + 1,\n",
    "            'cropType': crop,\n",
    "            'score': round(data['score'], 2),\n",
    "            'environment': [round(val, 2) for val in data['environment']]\n",
    "        })\n",
    "\n",
    "    total_computational_units = a_star.nodes_expanded + problem.distance_calculations\n",
    "\n",
    "    return {\n",
    "        'recommended_crop': recommended_crop,\n",
    "        'metrics': {\n",
    "            'nodesExpanded': a_star.nodes_expanded,\n",
    "            'maxFrontierSize': a_star.max_frontier_size,\n",
    "            'executionTime': round(a_star.execution_time, 3),\n",
    "        },\n",
    "        'rankings': ranked_crops\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c96638e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended crop: coffee\n",
      "\n",
      "Ranked crop types (lower score is better):\n",
      "Rank : 1 -> coffee with the score : 8.74\n",
      "Rank : 2 -> maize with the score : 18.81\n",
      "Rank : 3 -> jute with the score : 19.27\n",
      "Rank : 4 -> mango with the score : 20.22\n",
      "Rank : 5 -> pomegranate with the score : 22.25\n",
      "Rank : 6 -> papaya with the score : 22.36\n",
      "Rank : 7 -> orange with the score : 22.37\n",
      "Rank : 8 -> banana with the score : 22.78\n",
      "Rank : 9 -> cotton with the score : 23.83\n",
      "Rank : 10 -> coconut with the score : 25.07\n",
      "Rank : 11 -> pigeonpeas with the score : 27.06\n",
      "Rank : 12 -> blackgram with the score : 28.03\n",
      "Rank : 13 -> kidneybeans with the score : 28.06\n",
      "Rank : 14 -> watermelon with the score : 28.27\n",
      "Rank : 15 -> mothbeans with the score : 28.91\n",
      "Rank : 16 -> rice with the score : 30.26\n",
      "Rank : 17 -> muskmelon with the score : 34.76\n",
      "Rank : 18 -> lentil with the score : 36.25\n",
      "Rank : 19 -> mungbean with the score : 36.55\n",
      "Rank : 20 -> chickpea with the score : 42.71\n",
      "Rank : 21 -> apple with the score : 60.87\n",
      "Rank : 22 -> grapes with the score : 67.13\n",
      "{'recommended_crop': 'coffee', 'metrics': {'nodesExpanded': 53, 'maxFrontierSize': 4, 'executionTime': 0.043}, 'rankings': [{'rank': 1, 'cropType': 'coffee', 'score': np.float64(8.74), 'environment': [np.float64(108.0), np.float64(35.0), np.float64(25.0), np.float64(23.98), np.float64(61.11), np.float64(6.97), np.float64(161.53), np.float64(14.45), np.float64(2.0), np.float64(10.9), np.float64(8.41), np.float64(422.94), np.float64(7.35), np.float64(1.0), np.float64(14.65), np.float64(47.82), np.float64(108.46), np.float64(3.0), np.float64(36.83), np.float64(1.0), np.float64(48.35), np.float64(3.28)]}, {'rank': 2, 'cropType': 'maize', 'score': np.float64(18.81), 'environment': [np.float64(79.0), np.float64(45.0), np.float64(20.0), np.float64(23.81), np.float64(59.25), np.float64(5.72), np.float64(89.96), np.float64(20.04), np.float64(3.0), np.float64(9.8), np.float64(3.49), np.float64(361.79), np.float64(1.95), np.float64(3.0), np.float64(12.65), np.float64(10.36), np.float64(129.43), np.float64(2.0), np.float64(24.95), np.float64(3.0), np.float64(49.07), np.float64(3.83)]}, {'rank': 3, 'cropType': 'jute', 'score': np.float64(19.27), 'environment': [np.float64(73.0), np.float64(45.0), np.float64(37.0), np.float64(23.7), np.float64(74.64), np.float64(6.74), np.float64(181.28), np.float64(21.56), np.float64(3.0), np.float64(7.83), np.float64(10.84), np.float64(394.66), np.float64(8.57), np.float64(2.0), np.float64(11.39), np.float64(65.69), np.float64(146.46), np.float64(1.0), np.float64(22.22), np.float64(3.0), np.float64(38.22), np.float64(3.88)]}, {'rank': 4, 'cropType': 'mango', 'score': np.float64(20.22), 'environment': [np.float64(21.0), np.float64(21.0), np.float64(30.0), np.float64(27.7), np.float64(51.42), np.float64(5.4), np.float64(100.77), np.float64(14.82), np.float64(3.0), np.float64(11.13), np.float64(9.77), np.float64(433.36), np.float64(2.24), np.float64(1.0), np.float64(18.24), np.float64(74.05), np.float64(104.49), np.float64(2.0), np.float64(35.65), np.float64(1.0), np.float64(50.61), np.float64(2.59)]}, {'rank': 5, 'cropType': 'pomegranate', 'score': np.float64(22.25), 'environment': [np.float64(19.0), np.float64(17.0), np.float64(39.0), np.float64(24.72), np.float64(85.56), np.float64(6.73), np.float64(111.28), np.float64(27.33), np.float64(2.0), np.float64(10.66), np.float64(5.07), np.float64(382.24), np.float64(8.04), np.float64(3.0), np.float64(14.06), np.float64(97.95), np.float64(177.89), np.float64(2.0), np.float64(43.17), np.float64(2.0), np.float64(8.51), np.float64(4.22)]}, {'rank': 6, 'cropType': 'papaya', 'score': np.float64(22.36), 'environment': [np.float64(70.0), np.float64(54.0), np.float64(46.0), np.float64(39.73), np.float64(91.12), np.float64(6.92), np.float64(122.76), np.float64(17.25), np.float64(2.0), np.float64(10.64), np.float64(8.43), np.float64(380.79), np.float64(7.22), np.float64(2.0), np.float64(14.02), np.float64(40.4), np.float64(113.51), np.float64(2.0), np.float64(34.38), np.float64(2.0), np.float64(52.54), np.float64(2.88)]}, {'rank': 7, 'cropType': 'orange', 'score': np.float64(22.37), 'environment': [np.float64(21.0), np.float64(17.0), np.float64(15.0), np.float64(23.98), np.float64(91.55), np.float64(7.46), np.float64(118.49), np.float64(15.99), np.float64(2.0), np.float64(11.44), np.float64(1.16), np.float64(396.23), np.float64(5.22), np.float64(5.0), np.float64(18.15), np.float64(32.71), np.float64(110.31), np.float64(3.0), np.float64(14.87), np.float64(2.0), np.float64(35.95), np.float64(4.83)]}, {'rank': 8, 'cropType': 'banana', 'score': np.float64(22.78), 'environment': [np.float64(95.0), np.float64(75.0), np.float64(45.0), np.float64(28.98), np.float64(82.96), np.float64(5.83), np.float64(109.02), np.float64(21.27), np.float64(2.0), np.float64(11.99), np.float64(7.0), np.float64(367.16), np.float64(4.14), np.float64(2.0), np.float64(5.62), np.float64(5.94), np.float64(110.61), np.float64(1.0), np.float64(42.57), np.float64(1.0), np.float64(60.97), np.float64(4.75)]}, {'rank': 9, 'cropType': 'cotton', 'score': np.float64(23.83), 'environment': [np.float64(107.0), np.float64(36.0), np.float64(21.0), np.float64(25.29), np.float64(75.67), np.float64(6.21), np.float64(62.64), np.float64(13.75), np.float64(2.0), np.float64(8.58), np.float64(15.36), np.float64(352.18), np.float64(2.71), np.float64(2.0), np.float64(8.12), np.float64(14.45), np.float64(134.68), np.float64(3.0), np.float64(44.58), np.float64(3.0), np.float64(11.6), np.float64(4.57)]}, {'rank': 10, 'cropType': 'coconut', 'score': np.float64(25.07), 'environment': [np.float64(22.0), np.float64(16.0), np.float64(27.0), np.float64(29.18), np.float64(90.27), np.float64(6.01), np.float64(188.93), np.float64(21.96), np.float64(2.0), np.float64(8.55), np.float64(13.79), np.float64(407.79), np.float64(7.65), np.float64(1.0), np.float64(18.4), np.float64(51.5), np.float64(170.56), np.float64(3.0), np.float64(15.54), np.float64(1.0), np.float64(55.98), np.float64(2.84)]}, {'rank': 11, 'cropType': 'pigeonpeas', 'score': np.float64(27.06), 'environment': [np.float64(20.0), np.float64(74.0), np.float64(16.0), np.float64(36.04), np.float64(43.61), np.float64(4.76), np.float64(159.89), np.float64(16.83), np.float64(2.0), np.float64(8.06), np.float64(15.0), np.float64(402.83), np.float64(2.99), np.float64(2.0), np.float64(13.26), np.float64(51.97), np.float64(90.76), np.float64(2.0), np.float64(45.35), np.float64(2.0), np.float64(43.35), np.float64(4.09)]}, {'rank': 12, 'cropType': 'blackgram', 'score': np.float64(28.03), 'environment': [np.float64(50.0), np.float64(58.0), np.float64(23.0), np.float64(27.81), np.float64(62.5), np.float64(7.6), np.float64(69.76), np.float64(25.6), np.float64(1.0), np.float64(8.61), np.float64(17.03), np.float64(425.09), np.float64(5.82), np.float64(2.0), np.float64(15.13), np.float64(84.31), np.float64(89.94), np.float64(1.0), np.float64(39.22), np.float64(1.0), np.float64(18.1), np.float64(3.64)]}, {'rank': 13, 'cropType': 'kidneybeans', 'score': np.float64(28.06), 'environment': [np.float64(31.0), np.float64(55.0), np.float64(22.0), np.float64(22.91), np.float64(21.34), np.float64(5.87), np.float64(109.23), np.float64(17.82), np.float64(2.0), np.float64(9.82), np.float64(4.07), np.float64(409.81), np.float64(8.21), np.float64(2.0), np.float64(7.17), np.float64(39.76), np.float64(144.96), np.float64(2.0), np.float64(1.5), np.float64(2.0), np.float64(76.77), np.float64(2.5)]}, {'rank': 14, 'cropType': 'watermelon', 'score': np.float64(28.27), 'environment': [np.float64(97.0), np.float64(25.0), np.float64(50.0), np.float64(26.22), np.float64(80.9), np.float64(6.09), np.float64(49.09), np.float64(22.21), np.float64(2.0), np.float64(5.84), np.float64(0.41), np.float64(430.11), np.float64(5.56), np.float64(2.0), np.float64(16.16), np.float64(42.96), np.float64(80.51), np.float64(2.0), np.float64(40.17), np.float64(2.0), np.float64(57.73), np.float64(1.16)]}, {'rank': 15, 'cropType': 'mothbeans', 'score': np.float64(28.91), 'environment': [np.float64(24.0), np.float64(37.0), np.float64(21.0), np.float64(30.57), np.float64(58.23), np.float64(5.82), np.float64(62.75), np.float64(25.3), np.float64(2.0), np.float64(6.46), np.float64(9.12), np.float64(388.27), np.float64(2.0), np.float64(6.0), np.float64(5.2), np.float64(8.86), np.float64(193.5), np.float64(1.0), np.float64(8.25), np.float64(2.0), np.float64(58.61), np.float64(4.59)]}, {'rank': 16, 'cropType': 'rice', 'score': np.float64(30.26), 'environment': [np.float64(91.0), np.float64(50.0), np.float64(40.0), np.float64(20.82), np.float64(84.13), np.float64(6.46), np.float64(230.22), np.float64(12.51), np.float64(2.0), np.float64(10.67), np.float64(4.41), np.float64(396.47), np.float64(6.97), np.float64(5.0), np.float64(11.43), np.float64(45.21), np.float64(136.03), np.float64(2.0), np.float64(2.85), np.float64(2.0), np.float64(52.18), np.float64(1.17)]}, {'rank': 17, 'cropType': 'muskmelon', 'score': np.float64(34.76), 'environment': [np.float64(106.0), np.float64(20.0), np.float64(51.0), np.float64(29.73), np.float64(90.97), np.float64(6.34), np.float64(20.49), np.float64(14.32), np.float64(3.0), np.float64(9.56), np.float64(13.71), np.float64(384.22), np.float64(2.29), np.float64(3.0), np.float64(12.52), np.float64(73.71), np.float64(144.09), np.float64(2.0), np.float64(13.78), np.float64(1.0), np.float64(51.12), np.float64(1.28)]}, {'rank': 18, 'cropType': 'lentil', 'score': np.float64(36.25), 'environment': [np.float64(4.0), np.float64(61.0), np.float64(21.0), np.float64(24.84), np.float64(60.09), np.float64(6.75), np.float64(48.78), np.float64(22.4), np.float64(3.0), np.float64(9.92), np.float64(7.09), np.float64(389.67), np.float64(3.38), np.float64(1.0), np.float64(13.76), np.float64(26.54), np.float64(118.83), np.float64(1.0), np.float64(16.8), np.float64(3.0), np.float64(18.23), np.float64(2.23)]}, {'rank': 19, 'cropType': 'mungbean', 'score': np.float64(36.55), 'environment': [np.float64(25.0), np.float64(48.0), np.float64(21.0), np.float64(28.44), np.float64(83.49), np.float64(6.27), np.float64(52.55), np.float64(18.61), np.float64(2.0), np.float64(10.03), np.float64(11.36), np.float64(406.83), np.float64(4.21), np.float64(1.0), np.float64(11.27), np.float64(53.51), np.float64(113.35), np.float64(1.0), np.float64(11.7), np.float64(1.0), np.float64(80.96), np.float64(2.54)]}, {'rank': 20, 'cropType': 'chickpea', 'score': np.float64(42.71), 'environment': [np.float64(48.0), np.float64(65.0), np.float64(78.0), np.float64(17.44), np.float64(14.34), np.float64(7.86), np.float64(73.09), np.float64(26.67), np.float64(2.0), np.float64(7.53), np.float64(14.44), np.float64(416.76), np.float64(3.83), np.float64(6.0), np.float64(13.83), np.float64(55.82), np.float64(95.01), np.float64(3.0), np.float64(39.65), np.float64(1.0), np.float64(57.72), np.float64(3.98)]}, {'rank': 21, 'cropType': 'apple', 'score': np.float64(60.87), 'environment': [np.float64(10.0), np.float64(140.0), np.float64(197.0), np.float64(22.17), np.float64(90.27), np.float64(6.23), np.float64(124.47), np.float64(21.83), np.float64(3.0), np.float64(8.4), np.float64(7.42), np.float64(428.58), np.float64(1.76), np.float64(2.0), np.float64(15.77), np.float64(94.45), np.float64(169.18), np.float64(2.0), np.float64(31.16), np.float64(2.0), np.float64(92.56), np.float64(3.81)]}, {'rank': 22, 'cropType': 'grapes', 'score': np.float64(67.13), 'environment': [np.float64(8.0), np.float64(120.0), np.float64(196.0), np.float64(24.07), np.float64(82.66), np.float64(6.05), np.float64(69.82), np.float64(25.15), np.float64(1.0), np.float64(11.09), np.float64(5.24), np.float64(396.07), np.float64(7.22), np.float64(1.0), np.float64(16.53), np.float64(32.66), np.float64(124.79), np.float64(3.0), np.float64(35.45), np.float64(2.0), np.float64(53.31), np.float64(3.19)]}]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    file_path = 'data.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df.drop(df.columns[7], axis=1).to_numpy()  # Drop the crop column\n",
    "    y = df.iloc[:, 7].tolist()  # Crop labels\n",
    "    \n",
    "    user_input =104,18,30,23.60301571,60.39647474,6.779832611,140.9370415,23.911727578408552,3,8.639741540157758,14.481756579084477,413.1237161382283,6.401993710119095,3,14.652123227033409,3.5741911097655232,175.10424061806972,3,26.784996183697135,2,47.27126705890273,2.758819113898633\n",
    "    problem = CropsPredictionProblem(\n",
    "        state_space=X,\n",
    "        goal_state=user_input,\n",
    "        crop_labels=y\n",
    "    )\n",
    "\n",
    "    results = predict_best_crop(problem)\n",
    "    \n",
    "    print(f\"Recommended crop: {results['recommended_crop']}\")\n",
    "    print(\"\\nRanked crop types (lower score is better):\")\n",
    "    for x in results[\"rankings\"]:\n",
    "        print(f\"Rank : {x['rank']} -> {x['cropType']} with the score : {x['score']}\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ddb58e",
   "metadata": {},
   "source": [
    "# Visualize KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c582f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m         plt.show()\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m problem.knn_models:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     plot_knn_from_models(k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mplot_knn_from_models\u001b[39m\u001b[34m(knn_models)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_knn_from_models\u001b[39m(knn_models):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Plot KNN neighborhoods from trained models without needing original environments.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    - knn_models (dict): Trained NearestNeighbors models per crop type.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m crop_type, knn \u001b[38;5;129;01min\u001b[39;00m knn_models.items():\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m             X = knn._fit_X  \u001b[38;5;66;03m# Access stored training data (not officially public)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_knn_from_models(knn_models):\n",
    "    \"\"\"\n",
    "    Plot KNN neighborhoods from trained models without needing original environments.\n",
    "\n",
    "    Args:\n",
    "    - knn_models (dict): Trained NearestNeighbors models per crop type.\n",
    "    \"\"\"\n",
    "    for crop_type, knn in knn_models.items():\n",
    "        try:\n",
    "            X = knn._fit_X  # Access stored training data (not officially public)\n",
    "        except AttributeError:\n",
    "            print(f\"Model for {crop_type} has no stored data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        if len(X) < 2:\n",
    "            continue  # Not enough points to visualize\n",
    "\n",
    "        # Reduce to 2D for plotting\n",
    "        pca = PCA(n_components=2)\n",
    "        X_2d = pca.fit_transform(X)\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = knn.kneighbors(return_distance=False)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.title(f\"KNN for Crop: {crop_type}\")\n",
    "        plt.scatter(X_2d[:, 0], X_2d[:, 1], color='blue', label='Points')\n",
    "\n",
    "        # Draw neighbor lines\n",
    "        for i, nbrs in enumerate(neighbors):\n",
    "            for j in nbrs:\n",
    "                if i != j:\n",
    "                    plt.plot([X_2d[i, 0], X_2d[j, 0]], [X_2d[i, 1], X_2d[j, 1]], 'k--', linewidth=0.5)\n",
    "\n",
    "        plt.xlabel(\"PCA 1\")\n",
    "        plt.ylabel(\"PCA 2\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Now, plotting the KNN models from the `problem` object\n",
    "for crop_type, knn in problem.knn_models.items():\n",
    "    plot_knn_from_models({crop_type: knn})  # Pass each model as a dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
